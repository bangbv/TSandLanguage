# lightning.pytorch==2.1.2
trainer:
  accelerator: cuda
  strategy: ddp_find_unused_parameters_true
  devices: -1
  num_nodes: 1
  precision: bf16
  max_steps: 10
  limit_val_batches: 5
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: 0.25
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  log_every_n_steps: 100
  accumulate_grad_batches: 1
model:
  class_path: src.models.models.HFTranscriptionModel
  init_args:
    hf_name_or_path: openai/whisper-large 
    batch_size: 10
